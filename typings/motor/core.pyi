"""
This type stub file was generated by pyright.
"""

import pymongo
import pymongo.mongo_client
from pymongo.change_stream import ChangeStream
from pymongo.client_session import ClientSession
from pymongo.collection import Collection
from pymongo.command_cursor import CommandCursor, RawBatchCommandCursor
from pymongo.cursor import Cursor, RawBatchCursor
from pymongo.database import Database
from pymongo.encryption import ClientEncryption
from .metaprogramming import coroutine_annotation

"""Framework-agnostic core of Motor, an asynchronous driver for MongoDB."""
HAS_SSL = ...
_WITH_TRANSACTION_RETRY_TIME_LIMIT = ...
class AgnosticBase:
    def __eq__(self, other) -> bool:
        ...
    
    def __init__(self, delegate) -> None:
        ...
    
    def __repr__(self): # -> str:
        ...
    


class AgnosticBaseProperties(AgnosticBase):
    codec_options = ...
    read_preference = ...
    read_concern = ...
    write_concern = ...


class AgnosticClient(AgnosticBaseProperties):
    __motor_class_name__ = ...
    __delegate_class__ = pymongo.mongo_client.MongoClient
    address = ...
    arbiters = ...
    close = ...
    __hash__ = ...
    drop_database = ...
    options = ...
    get_database = ...
    get_default_database = ...
    HOST = ...
    is_mongos = ...
    is_primary = ...
    list_databases = ...
    list_database_names = ...
    nodes = ...
    PORT = ...
    primary = ...
    read_concern = ...
    secondaries = ...
    server_info = ...
    topology_description = ...
    start_session = ...
    def __init__(self, *args, **kwargs) -> None:
        """Create a new connection to a single MongoDB instance at *host:port*.

        Takes the same constructor arguments as
        :class:`~pymongo.mongo_client.MongoClient`, as well as:

        :Parameters:
          - `io_loop` (optional): Special event loop
            instance to use instead of default.
        """
        ...
    
    @property
    def io_loop(self):
        ...
    
    def get_io_loop(self):
        ...
    
    def watch(self, pipeline=..., full_document=..., resume_after=..., max_await_time_ms=..., batch_size=..., collation=..., start_at_operation_time=..., session=..., start_after=..., comment=...): # -> _:
        """Watch changes on this cluster.

        Returns a :class:`~MotorChangeStream` cursor which iterates over changes
        on all databases in this cluster. Introduced in MongoDB 4.0.

        See the documentation for :meth:`MotorCollection.watch` for more
        details and examples.

        :Parameters:
          - `pipeline` (optional): A list of aggregation pipeline stages to
            append to an initial ``$changeStream`` stage. Not all
            pipeline stages are valid after a ``$changeStream`` stage, see the
            MongoDB documentation on change streams for the supported stages.
          - `full_document` (optional): The fullDocument option to pass
            to the ``$changeStream`` stage. Allowed values: 'updateLookup'.
            When set to 'updateLookup', the change notification for partial
            updates will include both a delta describing the changes to the
            document, as well as a copy of the entire document that was
            changed from some time after the change occurred.
          - `resume_after` (optional): A resume token. If provided, the
            change stream will start returning changes that occur directly
            after the operation specified in the resume token. A resume token
            is the _id value of a change document.
          - `max_await_time_ms` (optional): The maximum time in milliseconds
            for the server to wait for changes before responding to a getMore
            operation.
          - `batch_size` (optional): The maximum number of documents to return
            per batch.
          - `collation` (optional): The :class:`~pymongo.collation.Collation`
            to use for the aggregation.
          - `start_at_operation_time` (optional): If provided, the resulting
            change stream will only return changes that occurred at or after
            the specified :class:`~bson.timestamp.Timestamp`. Requires
            MongoDB >= 4.0.
          - `session` (optional): a
            :class:`~pymongo.client_session.ClientSession`.
          - `start_after` (optional): The same as `resume_after` except that
            `start_after` can resume notifications after an invalidate event.
            This option and `resume_after` are mutually exclusive.
          - `comment` (optional): A user-provided comment to attach to this
            command.

        :Returns:
          A :class:`~MotorChangeStream`.

        .. versionchanged:: 3.0
           Added ``comment`` parameter.

        .. versionchanged:: 2.1
           Added the ``start_after`` parameter.

        .. versionadded:: 2.0

        .. mongodoc:: changeStreams
        """
        ...
    
    def __getattr__(self, name): # -> _:
        ...
    
    def __getitem__(self, name): # -> _:
        ...
    
    def wrap(self, obj): # -> _ | None:
        ...
    


class _MotorTransactionContext:
    """Internal transaction context manager for start_transaction."""
    def __init__(self, session) -> None:
        ...
    
    async def __aenter__(self): # -> Self@_MotorTransactionContext:
        ...
    
    async def __aexit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    


class AgnosticClientSession(AgnosticBase):
    """A session for ordering sequential operations.

    Do not create an instance of :class:`MotorClientSession` directly; use
    :meth:`MotorClient.start_session`:

    .. code-block:: python3

      collection = client.db.collection

      async with await client.start_session() as s:
          async with s.start_transaction():
              await collection.delete_one({'x': 1}, session=s)
              await collection.insert_one({'x': 2}, session=s)

    .. versionadded:: 2.0
    """
    __motor_class_name__ = ...
    __delegate_class__ = ClientSession
    commit_transaction = ...
    abort_transaction = ...
    end_session = ...
    cluster_time = ...
    has_ended = ...
    in_transaction = ...
    options = ...
    operation_time = ...
    session_id = ...
    advance_cluster_time = ...
    advance_operation_time = ...
    def __init__(self, delegate, motor_client) -> None:
        ...
    
    def get_io_loop(self):
        ...
    
    async def with_transaction(self, coro, read_concern=..., write_concern=..., read_preference=..., max_commit_time_ms=...):
        """Executes an awaitable in a transaction.

        This method starts a transaction on this session, awaits ``coro``
        once, and then commits the transaction. For example::

          async def coro(session):
              orders = session.client.db.orders
              inventory = session.client.db.inventory
              inserted_id = await orders.insert_one(
                  {"sku": "abc123", "qty": 100}, session=session)
              await inventory.update_one(
                  {"sku": "abc123", "qty": {"$gte": 100}},
                  {"$inc": {"qty": -100}}, session=session)
              return inserted_id

          async with await client.start_session() as session:
              inserted_id = await session.with_transaction(coro)

        To pass arbitrary arguments to the ``coro``, wrap it with a
        ``lambda`` like this::

          async def coro(session, custom_arg, custom_kwarg=None):
              # Transaction operations...

          async with await client.start_session() as session:
              await session.with_transaction(
                  lambda s: coro(s, "custom_arg", custom_kwarg=1))

        In the event of an exception, ``with_transaction`` may retry the commit
        or the entire transaction, therefore ``coro`` may be awaited
        multiple times by a single call to ``with_transaction``. Developers
        should be mindful of this possiblity when writing a ``coro`` that
        modifies application state or has any other side-effects.
        Note that even when the ``coro`` is invoked multiple times,
        ``with_transaction`` ensures that the transaction will be committed
        at-most-once on the server.

        The ``coro`` should not attempt to start new transactions, but
        should simply run operations meant to be contained within a
        transaction. The ``coro`` should also not commit the transaction;
        this is handled automatically by ``with_transaction``. If the
        ``coro`` does commit or abort the transaction without error,
        however, ``with_transaction`` will return without taking further
        action.

        When ``coro`` raises an exception, ``with_transaction``
        automatically aborts the current transaction. When ``coro`` or
        :meth:`~ClientSession.commit_transaction` raises an exception that
        includes the ``"TransientTransactionError"`` error label,
        ``with_transaction`` starts a new transaction and re-executes
        the ``coro``.

        When :meth:`~ClientSession.commit_transaction` raises an exception with
        the ``"UnknownTransactionCommitResult"`` error label,
        ``with_transaction`` retries the commit until the result of the
        transaction is known.

        This method will cease retrying after 120 seconds has elapsed. This
        timeout is not configurable and any exception raised by the
        ``coro`` or by :meth:`ClientSession.commit_transaction` after the
        timeout is reached will be re-raised. Applications that desire a
        different timeout duration should not use this method.

        :Parameters:
          - `coro`: The coroutine to run inside a transaction. The coroutine must
            accept a single argument, this session. Note, under certain error
            conditions the coroutine may be run multiple times.
          - `read_concern` (optional): The
            :class:`~pymongo.read_concern.ReadConcern` to use for this
            transaction.
          - `write_concern` (optional): The
            :class:`~pymongo.write_concern.WriteConcern` to use for this
            transaction.
          - `read_preference` (optional): The read preference to use for this
            transaction. If ``None`` (the default) the :attr:`read_preference`
            of this :class:`Database` is used. See
            :mod:`~pymongo.read_preferences` for options.

        :Returns:
          The return value of the ``coro``.

        .. versionadded:: 2.1
        """
        ...
    
    def start_transaction(self, read_concern=..., write_concern=..., read_preference=..., max_commit_time_ms=...): # -> _MotorTransactionContext:
        """Start a multi-statement transaction.

        Takes the same arguments as
        :class:`~pymongo.client_session.TransactionOptions`.

        Best used in a context manager block:

        .. code-block:: python3

          # Use "await" for start_session, but not for start_transaction.
          async with await client.start_session() as s:
              async with s.start_transaction():
                  await collection.delete_one({'x': 1}, session=s)
                  await collection.insert_one({'x': 2}, session=s)

        """
        ...
    
    @property
    def client(self): # -> Unknown:
        """The :class:`~MotorClient` this session was created from."""
        ...
    
    async def __aenter__(self): # -> Self@AgnosticClientSession:
        ...
    
    async def __aexit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    


class AgnosticDatabase(AgnosticBaseProperties):
    __motor_class_name__ = ...
    __delegate_class__ = Database
    __hash__ = ...
    __bool__ = ...
    command = ...
    create_collection = ...
    dereference = ...
    drop_collection = ...
    get_collection = ...
    list_collection_names = ...
    list_collections = ...
    name = ...
    validate_collection = ...
    with_options = ...
    _async_aggregate = ...
    def __init__(self, client, name, **kwargs) -> None:
        ...
    
    def aggregate(self, pipeline, *args, **kwargs): # -> _:
        """Execute an aggregation pipeline on this database.

        Introduced in MongoDB 3.6.

        The aggregation can be run on a secondary if the client is connected
        to a replica set and its ``read_preference`` is not :attr:`PRIMARY`.
        The :meth:`aggregate` method obeys the :attr:`read_preference` of this
        :class:`MotorDatabase`, except when ``$out`` or ``$merge`` are used, in
        which case  :attr:`PRIMARY` is used.

        All optional `aggregate command`_ parameters should be passed as
        keyword arguments to this method. Valid options include, but are not
        limited to:

          - `allowDiskUse` (bool): Enables writing to temporary files. When set
            to True, aggregation stages can write data to the _tmp subdirectory
            of the --dbpath directory. The default is False.
          - `maxTimeMS` (int): The maximum amount of time to allow the operation
            to run in milliseconds.
          - `batchSize` (int): The maximum number of documents to return per
            batch. Ignored if the connected mongod or mongos does not support
            returning aggregate results using a cursor.
          - `collation` (optional): An instance of
            :class:`~pymongo.collation.Collation`.
          - `let` (dict): A dict of parameter names and values. Values must be
            constant or closed expressions that do not reference document
            fields. Parameters can then be accessed as variables in an
            aggregate expression context (e.g. ``"$$var"``). This option is
            only supported on MongoDB >= 5.0.

        Returns a :class:`MotorCommandCursor` that can be iterated like a
        cursor from :meth:`find`::

           async def f():
               # Lists all operations currently running on the server.
               pipeline = [{"$currentOp": {}}]
               async for operation in client.admin.aggregate(pipeline):
                   print(operation)

        .. note:: This method does not support the 'explain' option. Please
           use :meth:`MotorDatabase.command` instead.

        .. note:: The :attr:`MotorDatabase.write_concern` of this database is
           automatically applied to this operation.

        .. versionadded:: 2.1

        .. _aggregate command:
            https://www.mongodb.com/docs/manual/reference/command/aggregate/
        """
        ...
    
    def watch(self, pipeline=..., full_document=..., resume_after=..., max_await_time_ms=..., batch_size=..., collation=..., start_at_operation_time=..., session=..., start_after=..., comment=...): # -> _:
        """Watch changes on this database.

        Returns a :class:`~MotorChangeStream` cursor which iterates over changes
        on this database. Introduced in MongoDB 4.0.

        See the documentation for :meth:`MotorCollection.watch` for more
        details and examples.

        :Parameters:
          - `pipeline` (optional): A list of aggregation pipeline stages to
            append to an initial ``$changeStream`` stage. Not all
            pipeline stages are valid after a ``$changeStream`` stage, see the
            MongoDB documentation on change streams for the supported stages.
          - `full_document` (optional): The fullDocument option to pass
            to the ``$changeStream`` stage. Allowed values: 'updateLookup'.
            When set to 'updateLookup', the change notification for partial
            updates will include both a delta describing the changes to the
            document, as well as a copy of the entire document that was
            changed from some time after the change occurred.
          - `resume_after` (optional): A resume token. If provided, the
            change stream will start returning changes that occur directly
            after the operation specified in the resume token. A resume token
            is the _id value of a change document.
          - `max_await_time_ms` (optional): The maximum time in milliseconds
            for the server to wait for changes before responding to a getMore
            operation.
          - `batch_size` (optional): The maximum number of documents to return
            per batch.
          - `collation` (optional): The :class:`~pymongo.collation.Collation`
            to use for the aggregation.
          - `start_at_operation_time` (optional): If provided, the resulting
            change stream will only return changes that occurred at or after
            the specified :class:`~bson.timestamp.Timestamp`. Requires
            MongoDB >= 4.0.
          - `session` (optional): a
            :class:`~pymongo.client_session.ClientSession`.
          - `start_after` (optional): The same as `resume_after` except that
            `start_after` can resume notifications after an invalidate event.
            This option and `resume_after` are mutually exclusive.
          - `comment` (optional): A user-provided comment to attach to this
            command.

        :Returns:
          A :class:`~MotorChangeStream`.

        .. versionchanged:: 3.0
           Added ``comment`` parameter.

        .. versionchanged:: 2.1
           Added the ``start_after`` parameter.

        .. versionadded:: 2.0

        .. mongodoc:: changeStreams
        """
        ...
    
    @property
    def client(self): # -> Unknown:
        """This MotorDatabase's :class:`MotorClient`."""
        ...
    
    def __getattr__(self, name): # -> _:
        ...
    
    def __getitem__(self, name): # -> _:
        ...
    
    def __call__(self, *args, **kwargs):
        ...
    
    def wrap(self, obj): # -> _ | Self@AgnosticDatabase:
        ...
    
    def get_io_loop(self):
        ...
    


class AgnosticCollection(AgnosticBaseProperties):
    __motor_class_name__ = ...
    __delegate_class__ = Collection
    __hash__ = ...
    __bool__ = ...
    bulk_write = ...
    count_documents = ...
    create_index = ...
    create_indexes = ...
    delete_many = ...
    delete_one = ...
    distinct = ...
    drop = ...
    drop_index = ...
    drop_indexes = ...
    estimated_document_count = ...
    find_one = ...
    find_one_and_delete = ...
    find_one_and_replace = ...
    find_one_and_update = ...
    full_name = ...
    index_information = ...
    insert_many = ...
    insert_one = ...
    name = ...
    options = ...
    rename = ...
    replace_one = ...
    update_many = ...
    update_one = ...
    with_options = ...
    _async_aggregate = ...
    _async_aggregate_raw_batches = ...
    _async_list_indexes = ...
    def __init__(self, database, name, codec_options=..., read_preference=..., write_concern=..., read_concern=..., _delegate=...) -> None:
        ...
    
    def __getattr__(self, name):
        ...
    
    def __getitem__(self, name):
        ...
    
    def __call__(self, *args, **kwargs):
        ...
    
    def find(self, *args, **kwargs): # -> _:
        """Create a :class:`MotorCursor`. Same parameters as for
        PyMongo's :meth:`~pymongo.collection.Collection.find`.

        Note that ``find`` does not require an ``await`` expression, because
        ``find`` merely creates a
        :class:`MotorCursor` without performing any operations on the server.
        ``MotorCursor`` methods such as :meth:`~MotorCursor.to_list`
        perform actual operations.
        """
        ...
    
    def find_raw_batches(self, *args, **kwargs): # -> _:
        """Query the database and retrieve batches of raw BSON.

        Similar to the :meth:`find` method but returns each batch as bytes.

        This example demonstrates how to work with raw batches, but in practice
        raw batches should be passed to an external library that can decode
        BSON into another data type, rather than used with PyMongo's
        :mod:`bson` module.

        .. code-block:: python3

          async def get_raw():
              cursor = db.test.find_raw_batches()
              async for batch in cursor:
                  print(bson.decode_all(batch))

        Note that ``find_raw_batches`` does not support sessions.

        .. versionadded:: 2.0
        """
        ...
    
    def aggregate(self, pipeline, *args, **kwargs): # -> _:
        """Execute an aggregation pipeline on this collection.

        The aggregation can be run on a secondary if the client is connected
        to a replica set and its ``read_preference`` is not :attr:`PRIMARY`.

        :Parameters:
          - `pipeline`: a single command or list of aggregation commands
          - `session` (optional): a
            :class:`~pymongo.client_session.ClientSession`, created with
            :meth:`~MotorClient.start_session`.
          - `**kwargs`: send arbitrary parameters to the aggregate command

        All optional `aggregate command`_ parameters should be passed as
        keyword arguments to this method. Valid options include, but are not
        limited to:

          - `allowDiskUse` (bool): Enables writing to temporary files. When set
            to True, aggregation stages can write data to the _tmp subdirectory
            of the --dbpath directory. The default is False.
          - `maxTimeMS` (int): The maximum amount of time to allow the operation
            to run in milliseconds.
          - `batchSize` (int): The maximum number of documents to return per
            batch. Ignored if the connected mongod or mongos does not support
            returning aggregate results using a cursor.
          - `collation` (optional): An instance of
            :class:`~pymongo.collation.Collation`.
          - `let` (dict): A dict of parameter names and values. Values must be
            constant or closed expressions that do not reference document
            fields. Parameters can then be accessed as variables in an
            aggregate expression context (e.g. ``"$$var"``). This option is
            only supported on MongoDB >= 5.0.

        Returns a :class:`MotorCommandCursor` that can be iterated like a
        cursor from :meth:`find`::

          async def f():
              pipeline = [{'$project': {'name': {'$toUpper': '$name'}}}]
              async for doc in collection.aggregate(pipeline):
                  print(doc)

        Note that this method returns a :class:`MotorCommandCursor` which
        lazily runs the aggregate command when first iterated. In order to run
        an aggregation with ``$out`` or ``$merge`` the application needs to
        iterate the cursor, for example::

           cursor = motor_coll.aggregate([{'$out': 'out'}])
           # Iterate the cursor to run the $out (or $merge) operation.
           await cursor.to_list(length=None)
           # Or more succinctly:
           await motor_coll.aggregate([{'$out': 'out'}]).to_list(length=None)
           # Or:
           async for _ in motor_coll.aggregate([{'$out': 'out'}]):
               pass

        :class:`MotorCommandCursor` does not allow the ``explain`` option. To
        explain MongoDB's query plan for the aggregation, use
        :meth:`MotorDatabase.command`::

          async def f():
              plan = await db.command(
                  'aggregate', 'COLLECTION-NAME',
                  pipeline=[{'$project': {'x': 1}}],
                  explain=True)

              print(plan)

        .. versionchanged:: 2.1
           This collection's read concern is now applied to pipelines
           containing the `$out` stage when connected to MongoDB >= 4.2.

        .. versionchanged:: 1.0
           :meth:`aggregate` now **always** returns a cursor.

        .. versionchanged:: 0.5
           :meth:`aggregate` now returns a cursor by default,
           and the cursor is returned immediately without an ``await``.
           See :ref:`aggregation changes in Motor 0.5 <aggregate_changes_0_5>`.

        .. versionchanged:: 0.2
           Added cursor support.

        .. _aggregate command:
            https://mongodb.com/docs/manual/applications/aggregation

        """
        ...
    
    def aggregate_raw_batches(self, pipeline, **kwargs): # -> _:
        """Perform an aggregation and retrieve batches of raw BSON.

        Similar to the :meth:`aggregate` method but returns each batch as bytes.

        This example demonstrates how to work with raw batches, but in practice
        raw batches should be passed to an external library that can decode
        BSON into another data type, rather than used with PyMongo's
        :mod:`bson` module.

        .. code-block:: python3

          async def get_raw():
              cursor = db.test.aggregate_raw_batches()
              async for batch in cursor:
                  print(bson.decode_all(batch))

        Note that ``aggregate_raw_batches`` does not support sessions.

        .. versionadded:: 2.0
        """
        ...
    
    def watch(self, pipeline=..., full_document=..., resume_after=..., max_await_time_ms=..., batch_size=..., collation=..., start_at_operation_time=..., session=..., start_after=..., comment=...): # -> _:
        """Watch changes on this collection.

        Performs an aggregation with an implicit initial ``$changeStream``
        stage and returns a :class:`~MotorChangeStream` cursor which
        iterates over changes on this collection.

        Introduced in MongoDB 3.6.

        A change stream continues waiting indefinitely for matching change
        events. Code like the following allows a program to cancel the change
        stream and exit.

        .. code-block:: python3

          change_stream = None

          async def watch_collection():
              global change_stream

              # Using the change stream in an "async with" block
              # ensures it is canceled promptly if your code breaks
              # from the loop or throws an exception.
              async with db.collection.watch() as change_stream:
                  async for change in change_stream:
                      print(change)

          # Tornado
          from tornado.ioloop import IOLoop

          def main():
              loop = IOLoop.current()
              # Start watching collection for changes.
            try:
                loop.run_sync(watch_collection)
            except KeyboardInterrupt:
                if change_stream:
                   loop.run_sync(change_stream.close)

          # asyncio
          try:
              asyncio.run(watch_collection())
          except KeyboardInterrupt:
              pass

        The :class:`~MotorChangeStream` async iterable blocks
        until the next change document is returned or an error is raised. If
        the :meth:`~MotorChangeStream.next` method encounters
        a network error when retrieving a batch from the server, it will
        automatically attempt to recreate the cursor such that no change
        events are missed. Any error encountered during the resume attempt
        indicates there may be an outage and will be raised.

        .. code-block:: python3

            try:
                pipeline = [{'$match': {'operationType': 'insert'}}]
                async with db.collection.watch(pipeline) as stream:
                    async for change in stream:
                        print(change)
            except pymongo.errors.PyMongoError:
                # The ChangeStream encountered an unrecoverable error or the
                # resume attempt failed to recreate the cursor.
                logging.error('...')

        For a precise description of the resume process see the
        `change streams specification`_.

        :Parameters:
          - `pipeline` (optional): A list of aggregation pipeline stages to
            append to an initial ``$changeStream`` stage. Not all
            pipeline stages are valid after a ``$changeStream`` stage, see the
            MongoDB documentation on change streams for the supported stages.
          - `full_document` (optional): The fullDocument option to pass
            to the ``$changeStream`` stage. Allowed values: 'updateLookup'.
            When set to 'updateLookup', the change notification for partial
            updates will include both a delta describing the changes to the
            document, as well as a copy of the entire document that was
            changed from some time after the change occurred.
          - `resume_after` (optional): A resume token. If provided, the
            change stream will start returning changes that occur directly
            after the operation specified in the resume token. A resume token
            is the _id value of a change document.
          - `max_await_time_ms` (optional): The maximum time in milliseconds
            for the server to wait for changes before responding to a getMore
            operation.
          - `batch_size` (optional): The maximum number of documents to return
            per batch.
          - `collation` (optional): The :class:`~pymongo.collation.Collation`
            to use for the aggregation.
          - `session` (optional): a
            :class:`~pymongo.client_session.ClientSession`.
          - `start_after` (optional): The same as `resume_after` except that
            `start_after` can resume notifications after an invalidate event.
            This option and `resume_after` are mutually exclusive.
          - `comment` (optional): A user-provided comment to attach to this
            command.

        :Returns:
          A :class:`~MotorChangeStream`.

        See the :ref:`tornado_change_stream_example`.

        .. versionchanged:: 3.0
           Added ``comment`` parameter.

        .. versionchanged:: 2.1
           Added the ``start_after`` parameter.

        .. versionadded:: 1.2

        .. mongodoc:: changeStreams

        .. _change streams specification:
            https://github.com/mongodb/specifications/blob/master/source/change-streams/change-streams.rst
        """
        ...
    
    def list_indexes(self, session=..., **kwargs): # -> _:
        """Get a cursor over the index documents for this collection. ::

          async def print_indexes():
              async for index in db.test.list_indexes():
                  print(index)

        If the only index is the default index on ``_id``, this might print::

            SON([('v', 1), ('key', SON([('_id', 1)])), ('name', '_id_')])
        """
        ...
    
    def wrap(self, obj): # -> Self@AgnosticCollection | AgnosticCursor | _:
        ...
    
    def get_io_loop(self):
        ...
    


class AgnosticBaseCursor(AgnosticBase):
    """Base class for AgnosticCursor and AgnosticCommandCursor"""
    _async_close = ...
    _refresh = ...
    address = ...
    cursor_id = ...
    alive = ...
    session = ...
    def __init__(self, cursor, collection) -> None:
        """Don't construct a cursor yourself, but acquire one from methods like
        :meth:`MotorCollection.find` or :meth:`MotorCollection.aggregate`.

        .. note::
          There is no need to manually close cursors; they are closed
          by the server after being fully iterated
          with :meth:`to_list`, :meth:`each`, or `async for`, or
          automatically closed by the client when the :class:`MotorCursor` is
          cleaned up by the garbage collector.
        """
        ...
    
    def __aiter__(self): # -> Self@AgnosticBaseCursor:
        ...
    
    async def next(self):
        """Advance the cursor.

        .. versionadded:: 2.2
        """
        ...
    
    __anext__ = ...
    @property
    @coroutine_annotation
    def fetch_next(self):
        """**DEPRECATED** - A Future used with `gen.coroutine`_ to
        asynchronously retrieve the next document in the result set,
        fetching a batch of documents from the server if necessary.
        Resolves to ``False`` if there are no more documents, otherwise
        :meth:`next_object` is guaranteed to return a document:

        .. doctest:: fetch_next
           :hide:

           >>> _ = MongoClient().test.test_collection.delete_many({})
           >>> collection = MotorClient().test.test_collection

        .. attention:: The :attr:`fetch_next` property is deprecated and will
           be removed in Motor 3.0. Use `async for` to iterate elegantly and
           efficiently over :class:`MotorCursor` objects instead.:

           .. doctest:: fetch_next

              >>> async def f():
              ...     await collection.drop()
              ...     await collection.insert_many([{'_id': i} for i in range(5)])
              ...     async for doc in collection.find():
              ...         sys.stdout.write(str(doc['_id']) + ', ')
              ...     print('done')
              ...
              >>> IOLoop.current().run_sync(f)
              0, 1, 2, 3, 4, done

        While it appears that fetch_next retrieves each document from
        the server individually, the cursor actually fetches documents
        efficiently in `large batches`_. Example usage:

        .. doctest:: fetch_next

           >>> async def f():
           ...     await collection.drop()
           ...     await collection.insert_many([{'_id': i} for i in range(5)])
           ...     cursor = collection.find().sort([('_id', 1)])
           ...     while (await cursor.fetch_next):
           ...         doc = cursor.next_object()
           ...         sys.stdout.write(str(doc['_id']) + ', ')
           ...     print('done')
           ...
           >>> IOLoop.current().run_sync(f)
           0, 1, 2, 3, 4, done

        .. versionchanged:: 2.2
           Deprecated.

        .. _`large batches`: https://www.mongodb.com/docs/manual/tutorial/iterate-a-cursor/#cursor-batches
        .. _`gen.coroutine`: http://tornadoweb.org/en/stable/gen.html
        """
        ...
    
    def next_object(self): # -> None:
        """**DEPRECATED** - Get a document from the most recently fetched
        batch, or ``None``. See :attr:`fetch_next`.

        The :meth:`next_object` method is deprecated and may be removed
        in a future major release. Use `async for` to elegantly iterate over
        :class:`MotorCursor` objects instead.

        .. versionchanged:: 2.2
           Deprecated.
        """
        ...
    
    def each(self, callback): # -> None:
        """Iterates over all the documents for this cursor.

        :meth:`each` returns immediately, and `callback` is executed asynchronously
        for each document. `callback` is passed ``(None, None)`` when iteration
        is complete.

        Cancel iteration early by returning ``False`` from the callback. (Only
        ``False`` cancels iteration: returning ``None`` or 0 does not.)

        .. testsetup:: each

           from tornado.ioloop import IOLoop
           MongoClient().test.test_collection.delete_many({})
           MongoClient().test.test_collection.insert_many(
               [{'_id': i} for i in range(5)])

           collection = MotorClient().test.test_collection

        .. doctest:: each

           >>> def each(result, error):
           ...     if error:
           ...         raise error
           ...     elif result:
           ...         sys.stdout.write(str(result['_id']) + ', ')
           ...     else:
           ...         # Iteration complete
           ...         IOLoop.current().stop()
           ...         print('done')
           ...
           >>> cursor = collection.find().sort([('_id', 1)])
           >>> cursor.each(callback=each)
           >>> IOLoop.current().start()
           0, 1, 2, 3, 4, done

        .. note:: Unlike other Motor methods, ``each`` requires a callback and
           does not return a Future, so it cannot be used in a coroutine.
           ``async for`` and :meth:`to_list` are much easier to use.

        :Parameters:
         - `callback`: function taking (document, error)
        """
        ...
    
    @coroutine_annotation
    def to_list(self, length):
        """Get a list of documents.

        .. testsetup:: to_list

          MongoClient().test.test_collection.delete_many({})
          MongoClient().test.test_collection.insert_many([{'_id': i} for i in range(4)])

          from tornado import ioloop

        .. doctest:: to_list

          >>> from motor.motor_tornado import MotorClient
          >>> collection = MotorClient().test.test_collection
          >>>
          >>> async def f():
          ...     cursor = collection.find().sort([('_id', 1)])
          ...     docs = await cursor.to_list(length=2)
          ...     while docs:
          ...         print(docs)
          ...         docs = await cursor.to_list(length=2)
          ...
          ...     print('done')
          ...
          >>> ioloop.IOLoop.current().run_sync(f)
          [{'_id': 0}, {'_id': 1}]
          [{'_id': 2}, {'_id': 3}]
          done

        :Parameters:
         - `length`: maximum number of documents to return for this call, or
           None

         Returns a Future.

        .. versionchanged:: 2.0
           No longer accepts a callback argument.

        .. versionchanged:: 0.2
           `callback` must be passed as a keyword argument, like
           ``to_list(10, callback=callback)``, and the
           `length` parameter is no longer optional.
        """
        ...
    
    def get_io_loop(self):
        ...
    
    async def close(self): # -> None:
        """Explicitly kill this cursor on the server.

        Call like::

            await cursor.close()

        """
        ...
    
    def batch_size(self, batch_size): # -> Self@AgnosticBaseCursor:
        ...
    


class AgnosticCursor(AgnosticBaseCursor):
    __motor_class_name__ = ...
    __delegate_class__ = Cursor
    address = ...
    collation = ...
    distinct = ...
    explain = ...
    add_option = ...
    remove_option = ...
    limit = ...
    skip = ...
    max_scan = ...
    sort = ...
    hint = ...
    where = ...
    max_await_time_ms = ...
    max_time_ms = ...
    min = ...
    max = ...
    comment = ...
    allow_disk_use = ...
    _Cursor__die = ...
    def rewind(self): # -> Self@AgnosticCursor:
        """Rewind this cursor to its unevaluated state."""
        ...
    
    def clone(self): # -> Self@AgnosticCursor:
        """Get a clone of this cursor."""
        ...
    
    def __copy__(self): # -> Self@AgnosticCursor:
        ...
    
    def __deepcopy__(self, memo): # -> Self@AgnosticCursor:
        ...
    


class AgnosticRawBatchCursor(AgnosticCursor):
    __motor_class_name__ = ...
    __delegate_class__ = RawBatchCursor


class AgnosticCommandCursor(AgnosticBaseCursor):
    __motor_class_name__ = ...
    __delegate_class__ = CommandCursor
    _CommandCursor__die = ...


class AgnosticRawBatchCommandCursor(AgnosticCommandCursor):
    __motor_class_name__ = ...
    __delegate_class__ = RawBatchCommandCursor


class _LatentCursor:
    """Take the place of a PyMongo CommandCursor until aggregate() begins."""
    alive = ...
    _CommandCursor__data = ...
    _CommandCursor__id = ...
    _CommandCursor__killed = ...
    _CommandCursor__sock_mgr = ...
    _CommandCursor__session = ...
    _CommandCursor__explicit_session = ...
    cursor_id = ...
    def __init__(self, collection) -> None:
        ...
    
    def clone(self): # -> _LatentCursor:
        ...
    
    def rewind(self): # -> None:
        ...
    


class AgnosticLatentCommandCursor(AgnosticCommandCursor):
    __motor_class_name__ = ...
    def __init__(self, collection, start, *args, **kwargs) -> None:
        ...
    
    def batch_size(self, batch_size): # -> Self@AgnosticLatentCommandCursor:
        ...
    


class AgnosticChangeStream(AgnosticBase):
    """A change stream cursor.

    Should not be called directly by application developers. See
    :meth:`~MotorCollection.watch` for example usage.

    .. versionadded: 1.2
    .. mongodoc:: changeStreams
    """
    __delegate_class__ = ChangeStream
    __motor_class_name__ = ...
    _close = ...
    resume_token = ...
    def __init__(self, target, pipeline, full_document, resume_after, max_await_time_ms, batch_size, collation, start_at_operation_time, session, start_after, comment) -> None:
        ...
    
    @property
    def alive(self): # -> Literal[True]:
        """Does this cursor have the potential to return more data?

        .. note:: Even if :attr:`alive` is ``True``, :meth:`next` can raise
            :exc:`StopAsyncIteration` and :meth:`try_next` can return ``None``.

        """
        ...
    
    async def next(self):
        """Advance the cursor.

        This method blocks until the next change document is returned or an
        unrecoverable error is raised. This method is used when iterating over
        all changes in the cursor. For example::

            async def watch_collection():
                resume_token = None
                pipeline = [{'$match': {'operationType': 'insert'}}]
                try:
                    async with db.collection.watch(pipeline) as stream:
                        async for insert_change in stream:
                            print(insert_change)
                            resume_token = stream.resume_token
                except pymongo.errors.PyMongoError:
                    # The ChangeStream encountered an unrecoverable error or the
                    # resume attempt failed to recreate the cursor.
                    if resume_token is None:
                        # There is no usable resume token because there was a
                        # failure during ChangeStream initialization.
                        logging.error('...')
                    else:
                        # Use the interrupted ChangeStream's resume token to
                        # create a new ChangeStream. The new stream will
                        # continue from the last seen insert change without
                        # missing any events.
                        async with db.collection.watch(
                                pipeline, resume_after=resume_token) as stream:
                            async for insert_change in stream:
                                print(insert_change)

        Raises :exc:`StopAsyncIteration` if this change stream is closed.

        In addition to using an "async for" loop as shown in the code
        example above, you can also iterate the change stream by calling
        ``await change_stream.next()`` repeatedly.
        """
        ...
    
    async def try_next(self):
        """Advance the cursor without blocking indefinitely.

        This method returns the next change document without waiting
        indefinitely for the next change. If no changes are available,
        it returns None. For example:

        .. code-block:: python3

          while change_stream.alive:
              change = await change_stream.try_next()
              # Note that the ChangeStream's resume token may be updated
              # even when no changes are returned.
              print("Current resume token: %r" % (change_stream.resume_token,))
              if change is not None:
                  print("Change document: %r" % (change,))
                  continue
              # We end up here when there are no recent changes.
              # Sleep for a while before trying again to avoid flooding
              # the server with getMore requests when no changes are
              # available.
              await asyncio.sleep(10)

        If no change document is cached locally then this method runs a single
        getMore command. If the getMore yields any documents, the next
        document is returned, otherwise, if the getMore returns no documents
        (because there have been no changes) then ``None`` is returned.

        :Returns:
          The next change document or ``None`` when no document is available
          after running a single getMore or when the cursor is closed.

        .. versionadded:: 2.1
        """
        ...
    
    async def close(self): # -> None:
        """Close this change stream.

        Stops any "async for" loops using this change stream.
        """
        ...
    
    def __aiter__(self): # -> Self@AgnosticChangeStream:
        ...
    
    __anext__ = ...
    async def __aenter__(self): # -> Self@AgnosticChangeStream:
        ...
    
    async def __aexit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    
    def get_io_loop(self):
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    


class AgnosticClientEncryption(AgnosticBase):
    """Explicit client-side field level encryption."""
    __motor_class_name__ = ...
    __delegate_class__ = ClientEncryption
    create_data_key = ...
    encrypt = ...
    decrypt = ...
    close = ...
    def __init__(self, kms_providers, key_vault_namespace, key_vault_client, codec_options, io_loop=...) -> None:
        """Explicit client-side field level encryption.

        Takes the same constructor arguments as
        :class:`pymongo.encryption.ClientEncryption`, as well as:

        :Parameters:
          - `io_loop` (optional): Special event loop
            instance to use instead of default.
        """
        ...
    
    @property
    def io_loop(self):
        ...
    
    def get_io_loop(self):
        ...
    
    async def __aenter__(self): # -> Self@AgnosticClientEncryption:
        ...
    
    async def __aexit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    


